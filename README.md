# Deepeval for Evaluating Copilot LLM Responses

This project uses [Deepeval](https://github.com/confident-ai/deepeval) to write tests and evaluate the responses of Copilot LLM.
The purpose of evaluating Copilot LLM responses is to assess the performance of the Language Model in generating code based on different prompts. By evaluating the responses, you can understand how well the model is interpreting and responding to various prompts. This can help in identifying areas where the model's performance can be improved.
The tests focus on evaluating the responses generated by Copilot LLM and outputs a score indicating the accuracy and relevance of the responses.

## Getting Started

To get started with this project, you need to clone the repository to your local machine:

```bash
git clone git@github.com:vudayani-vmw/DeepEvalTests.git
```

## Prerequisites

Before running the tests, ensure you have the following installed:

- Python 3.8 or higher. If not installed, you can download it from [here](https://www.python.org/downloads/).
- Pip (Python package installer)
- Deepeval package: Install it using pip `pip install -U deepeval`

## Running the Tests

To run the tests, execute the following command:

```bash
deepeval test run test_g_eval_metric.py
```

## Running as a Docker Service

An endpoint is now exposed to run Deepeval tests, allowing other services to interact with this application. You can run the application as a Docker service:

1. Build the Docker image:

```bash
docker build -t deepeval-service .
```

1. Run the Docker container, exposing it on port 8000 and setting the necessary environment variable:

```bash
docker run -p 8000:8000 -e <OPENAI_API_KEY> deepeval-service
```

With the container running, the Deepeval service will be accessible at http://localhost:8000, ready to evaluate Copilot LLM responses through the exposed endpoints.

## Test case

The test case in this project takes in expected output and actual output, and uses the G-Eval metric of deepeval to evaluate the response based on provided criteria. The test case is defined in the `test_g_eval_metric.py` file.

Here's a brief overview of how the test case works:

1. It reads the actual and expected outputs from specific files.
2. It defines a test case function (`test_case_boot_3`) that sets up a G-Eval metric.
3. This metric is used to evaluate the correctness of the actual output against the expected output based on specific evaluation steps.

**Note**: The test case assumes that the actual and expected output files are located in the same level of directory as this test project

**Note**: To learn more about G-Eval Metric, refer the documentation [here](https://docs.confident-ai.com/docs/metrics-llm-evals)